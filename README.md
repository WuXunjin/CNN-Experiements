# Evaluating CNN Predictions using Visualization Techniques
Convolutional Neural Network models have produced remarkable
results in image classification. Nevertheless, there is no clear
understanding of why they perform so well. We use two different
techniques that offer visual data to understand Convolutional
Neural Networks. These approaches are known as Occlusion
Sensitivity and Gradient-Weighted Class Activation Mapping. We
also collect data from humans to configure what portions of an
image they use to make decisions with respect to classification. We
compare the results from Occlusion Sensitivity, Class Activation
Mapping and human data, and find out that CNN classify images
based on the similar portions that human used to classify things.
Moreover, we also find out that CNN behaves differently when
classifying the irregular inputs, such as adversarial examples and
noise images. We give some possible explanations and believe that
further research is needed to explain this different behavior.
